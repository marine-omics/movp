// This config file was develop to work on setonix, ~800 samples, and a genome size of 500 Mb

// Setonix specific constraints mainly represent the maximum number of inodes (files)
// This in turns affects the gatk_chunksize parameter (which can be set at the end of this file).
// The current chunk size achieves a balanace between number of inodes and memory usage for genotypeGVCF step
// If you have a much larger genome you will need to increase the chunk size to limit the number of inodes
// If that fails, I recommend running MOVP one chromosome at a time

// Sample specific costraints mostly represent the memory needed for the multi sample genotypeGVCF step
// If you have less than 800 samples, you can reduce the memory for multi sample steps (e.g., genotypeGVCF)
// If you have more samples, (more than 1000?) you might struggle to get nodes with enough memory
// If that's the case I suggest splitting by chromosome, and reducing the chunksize.
      
process {
  scratch = true

  queue = 'work'
  cpus = 1
  memory = 4.GB
  time = '1h'
  errorStrategy = 'retry'
  maxRetries = 1

  withName: 'bwa_index'{
    time = '4h'
    memory = 4.GB
  }

// QC
// Used here entirely just for polyG trimming.  We will adapter trim later using MarkIlluminaAdapters assuming that this works better if data remains untrimmed at this point
  withName: 'fastp' {
    time = '4h'
    memory = 8.GB
    cpus = 4
    ext.args = '--trim_poly_g  --dont_eval_duplication --disable_adapter_trimming'
  }

  withName: 'fastqc' {
    time = '4h'
  }

// changed from 20.GB to 10.GB
  withName: 'multiqc_fastqc'{
    memory = 10.GB
    time = '10h'
  }

  withName: 'multiqc_fastp'{
    memory = 20.GB
    time = '10h'
  }

// Read Preparation
// changed from 6.GB to 10.GB
  withName: 'markadapters'{
    cpus = 2
    memory = 10.GB
  }

  withName: 'fastq2ubam'{
	  memory = { 30.GB * task.attempt }
    cpus = 2
  }

// Mapping
// changed from 50 to 20. failed for 1 sample, changed back to 30
// NOTE: when I provided uneven numbers, it failed saying the requested amount/2 was not valid
// if i provided 25 Gb, if failed saying that 12.5 was not valid
  withName: 'bwa_mem_gatk'{
    cpus = 4
    memory = { 30.GB * task.attempt }
    time = '23h'
  }

  withName: 'name_by_sample'{
    memory = { 5.GB * task.attempt }
  }

  withName: 'samtools_merge'{
    cpus = 1
    memory = 20.GB
    time = '10h'
  }

  withName: 'stat'{
    cpus = 1
    memory = 4.GB
  }

// Duplicate marking
//
  withName: 'gatk_mark_duplicates'{
    memory = { 70.GB * task.attempt }
    time = '23h'
  }

// variant calling and genotyping
// changed haplo caller from 23 to 3 hours as it does not require that much
// Note, that is with small (15 Mb) intervals, larger intervals will require more time
  withName: 'gatk_haplotype_caller'{
	cpus = 8
	memory = { 8.GB * task.attempt }
	time = '3h'
  }

  withName: 'gatk_genotypegvcfs'{
  	memory = { 220.GB }
	time = '23h'
	scratch = true
  }

// changed from 80 to 100 GB, test
// change from 23 to 12 hours for window of 14 Mb
  withName: 'gatk_genomicsdb_import'{
	memory = { 100.GB }
	time = '12h'
  }

// changed from 8 to 1 cpus and from 120 to 220 GB
// Above parameters were for merge vcfs and always failed.
// current memory works for gather vcfs
  withName: 'gatk_gathervcfs'{
    memory = { 120.GB }
    time = '23h'
    cpus = 1
  }

  withName: 'mpileup_call'{
    cpus = 1
    memory = 10.GB
    ext.args='-D -q 20 -Q 20 -a FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,FORMAT/SCR'
    ext.callargs='-c -f GQ'
  }

  withName: 'freebayes'{
    memory = { 20.GB * task.attempt }
    time = '12h'
    cpus = 2
  }


}


// Increase chunksize for gatk
// originally ran with 50,000,000, files were too large
// Re ran with 5,000,000, produced too many files
// Re ran with 20,000,000, too high mem usage for genotypeGVFs
// Re ran with 20,000,000 and java_heap_fraction, and it ran for too long (> 23h, killed)
// this avoids producing too many intermediate files
params {
    gatk_chunksize = 14500000
    java_heap_fraction = 0.7
}
